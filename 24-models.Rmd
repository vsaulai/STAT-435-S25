---
title: "Model Building"
author: "STAT 345 - Statistical Computing"
output:
  ioslides_presentation: default
  beamer_presentation: default
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(out.width = '80%', collapse = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(png)
```

## Agenda

- Exploration vs. confirmation
- Basic modeling
- Optimization based on distance metrics
- Using models for exploration

## Exploration vs confirmation

- Each observation can either be used for exploration or confirmation, not both.
- You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. 
- As soon as you use an observation twice, you’ve switched from confirmation to exploration.

## Splitting data

Wickham recommends splitting your data into three pieces before you begin the analysis:

- 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.
- 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.
- 20% is held back for a test set. You can only use this data ONCE, to test your final model.

## Modeling process

- Choose a family of models (e.g., straight line, quadratic curve)
- Generate a fitted model -- this represents the "best" model of your chosen family of models (and that's it, actually...)
  - "Best" is subjective, and depends on how you measure "best"
  - Your chosen family of models is wrong anyway!

## modelr

```{r}
library(modelr) #wrapper for models, works with pipe
options(na.action = na.warn)
```

## A simple relationship

```{r fig.height=3}
#sim1 part of the modelr package
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

- Reasonable first choice: a straight-line relationship
- Need to choose a slope and an intercept to define the relationship

## Trying models

```{r fig.height=3}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point() 
```

## Trying models

- Some of those lines were close to what we might expect the relationship to be. 
- Some were way off!
- How do we choose which line is the most appropriate?
  - Need to define a "distance metric" that somehow measures how far each point is from a given line (model)
  - "Linear Regression" uses squared vertical distance of a point from the line (i.e., the "squared residuals")

## Distance (residual)

```{r}
dist1 <- sim1 %>% 
  mutate(
    #add a little jitter to the x for plotting
    dodge = rep(c(-1, 0, 1) / 20, 10), 
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )
```

- This adds a variable called "pred" based on the relationship $7 + 1.5x$

## Distance (residual) 

```{r echo=FALSE}
ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "#3366FF") 
```



## Predicted values

Turn the model family into a function:

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
```

## RMSD (RMSE)

Compute the difference between actual and predicted, square them, average them, and the take the square root.

- This is a little different than the "residual sums of squares" often used in regression 

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```

## Multiple models

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

## 10 best models

```{r echo=FALSE}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

## Models as observations

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

## Grid search

```{r eval=FALSE}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

## Grid search

```{r echo=FALSE}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

## Grid search

```{r echo=FALSE}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

## Basic optimization

```{r}
#Newton-Raphson optimization
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
```

## Basic optimization

```{r echo=FALSE}

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

## lm

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

`lm` doesn't actually use `optim` -- it uses properties you see in STAT 442

## Many models

```{r}
library(gapminder)
gapminder
```


## Motivating question

How does life expectancy (`lifeExp`) change over time (`year`) for each country (`country`)?

```{r fig.height=3}
gapminder %>% 
  ggplot(aes(year, lifeExp, group = country)) +
    geom_line(alpha = 1/3)
```

## New Zealand

- `add_predictions()` creates `pred` given a model, and 
- `add_residuals()` creates `resid` based on the model

```{r, out.width = "33%", fig.asp = 1, fig.width = 3, fig.align='default', echo=FALSE}
nz <- filter(gapminder, country == "New Zealand")
nz %>% 
  ggplot(aes(year, lifeExp)) + 
  geom_line() + 
  ggtitle("Full data = ")
nz_mod <- lm(lifeExp ~ year, data = nz)
nz %>% 
  add_predictions(nz_mod) %>%
  ggplot(aes(year, pred)) + 
  geom_line() + 
  ggtitle("Linear trend + ")
nz %>% 
  add_residuals(nz_mod) %>% 
  ggplot(aes(year, resid)) + 
  geom_hline(yintercept = 0, colour = "white", size = 3) + 
  geom_line() + 
  ggtitle("Remaining pattern")
```

## Nested data

```{r}
by_country <- gapminder %>% 
  group_by(country, continent) %>% 
  nest()
by_country
```

## Nested data

```{r}
by_country$data[[1]]
```

## Extending to all countries

```{r}
country_model <- function(df) {
  lm(lifeExp ~ year, data = df)
}
```

```{r}
by_country <- by_country %>% 
  mutate(model = map(data, country_model)) %>%
  mutate(resids = map2(data, model, add_residuals))
by_country
```

## Unnesting

```{r}
resids <- unnest(by_country, resids)
resids
```

## Back to plotting

```{r}
resids %>% 
  ggplot(aes(year, resid)) +
    geom_line(aes(group = country), alpha = 1 / 3) + 
    geom_smooth(se = FALSE)
```

## Back to plotting

Facetting by continent is particularly revealing:
 
```{r}
resids %>% 
  ggplot(aes(year, resid, group = country)) +
    geom_line(alpha = 1 / 3) + 
    facet_wrap(~continent)
```

## broom: models to tidy data

```{r}
broom::glance(nz_mod)
```

## broom: models to tidy data

```{r}
glance <- by_country %>% 
  mutate(glance = map(model, broom::glance)) %>% 
  unnest(glance)
```

## Looking for bad fit

```{r}
glance %>% 
  arrange(r.squared)
```

## Jitter

```{r fig.height=3}
glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_jitter(width = 0.5)
```

## Bad fits

```{r echo=FALSE}
bad_fit <- filter(glance, r.squared < 0.25)

gapminder %>% 
  semi_join(bad_fit, by = "country") %>% 
  ggplot(aes(year, lifeExp, colour = country)) +
    geom_line()
```


## Common broom functions

- `broom::glance(model)` returns a row for each model. Each column gives a model summary: either a measure of model quality, or complexity, or a combination of the two.
- `broom::tidy(model)` returns a row for each coefficient in the model. Each column gives information about the estimate or its variability.
- `broom::augment(model, data)` returns a row for each row in data, adding extra values like residuals, and influence statistics.